{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a786c77c",
   "metadata": {},
   "source": [
    "# LangChain: Memory\n",
    "\n",
    "\n",
    "## [DEPRECATED] Outline\n",
    "* ConversationBufferMemory\n",
    "* ConversationBufferWindowMemory\n",
    "* ConversationTokenBufferMemory\n",
    "* ConversationSummaryMemory\n",
    "\n",
    "## LangChainDeprecationWarning Found\n",
    "\n",
    "In the new version of LangChain (0.3.4) all these memory classes and others too are deprecated:\n",
    "* LangChainDeprecationWarning: \n",
    "    * The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. \n",
    "    * An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. \n",
    "    * To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
    "        * llm = ChatOpenAI(temperature=0.0, model=llm_model)\n",
    "* LangChainDeprecationWarning: \n",
    "    * Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
    "        * memory = ConversationBufferMemory()\n",
    "* LangChainDeprecationWarning: \n",
    "    * The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. \n",
    "    * Use :meth:`~RunnableWithMessageHistory: https://python.langchain.com/v0.2/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html` instead.\n",
    "        * conversation = ConversationChain("
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1297dcd5",
   "metadata": {},
   "source": [
    "## ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1f518f5",
   "metadata": {
    "height": 132,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "_ = load_dotenv()\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f920989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Side note:\n",
    "\n",
    "from getpass import getpass\n",
    "x = getpass(\"OPENAI API KEY: \")\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2301270b-7a69-40f8-9934-f840499b6ae9",
   "metadata": {},
   "source": [
    "Note: LLM's do not always produce the same results. When executing the code in your notebook, you may get slightly different answers that those in the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce0924a2-ce2d-44ba-a806-c1195720c237",
   "metadata": {
    "height": 249
   },
   "outputs": [],
   "source": [
    "# Today: 2024-10-22\n",
    "# The gpt-3.5-turbo currently points to gpt-3.5-turbo-0125.\n",
    "# Let's use the defauld selection from the openai integration.\n",
    "#llm_model = \"gpt-3.5-turbo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20ad6fe2",
   "metadata": {
    "height": 81,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from langchain.chat_models import ChatOpenAI\n",
    "# from langchain.chains import ConversationChain\n",
    "# from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88bdf13d",
   "metadata": {
    "height": 132,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# LangChainDeprecationWarning: \n",
    "#   The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. \n",
    "#   An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. \n",
    "#   To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
    "#\n",
    "#     llm = ChatOpenAI(temperature=0.0, model=llm_model)\n",
    "\n",
    "# LangChainDeprecationWarning: \n",
    "#   Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
    "#\n",
    "#     memory = ConversationBufferMemory()\n",
    "\n",
    "# LangChainDeprecationWarning: \n",
    "#   The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. \n",
    "#   Use :meth:`~RunnableWithMessageHistory: https://python.langchain.com/v0.2/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html` instead.\n",
    "#\n",
    "#     conversation = ConversationChain(\n",
    "\n",
    "# [DEPRECATED]\n",
    "\n",
    "#llm = ChatOpenAI(temperature=0.0, model=llm_model)\n",
    "#memory = ConversationBufferMemory()\n",
    "#conversation = ConversationChain(\n",
    "#    llm=llm, \n",
    "#    memory = memory,\n",
    "#    verbose=True\n",
    "#)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ec99a1",
   "metadata": {},
   "source": [
    "### Let's rewrite the code to add the histoty manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6fea229b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ffa1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a valuable assistant.\"),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{input}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "prompt_value = prompt.invoke(\n",
    "    {\n",
    "        \"history\": [(\"human\", \"what's 5 + 2\"), (\"ai\", \"5 + 2 is 7\")],\n",
    "        \"input\": \"now multiply that by 4\"\n",
    "    }\n",
    ")\n",
    "\n",
    "prompt_value.to_string()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f88655ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI()\n",
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5370498e",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acba6cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = \"what's 5 + 2\"\n",
    "ai_msg = chain.invoke({\"history\": history, \"input\": input})\n",
    "history.extend([HumanMessage(content=input), ai_msg])\n",
    "print(ai_msg.content)\n",
    "print(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95518193",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = \"now multiply that by 4\"\n",
    "ai_msg = chain.invoke({\"history\": history, \"input\": input})\n",
    "history.extend([HumanMessage(content=input), ai_msg])\n",
    "print(ai_msg.content)\n",
    "print(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce427449",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = \"by the way, my name is Hammer ... what's your name?\"\n",
    "ai_msg = chain.invoke({\"history\": history, \"input\": input})\n",
    "history.extend([HumanMessage(content=input), ai_msg])\n",
    "print(ai_msg.content)\n",
    "print(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a450bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = \"what was the last math operation we did?\"\n",
    "ai_msg = chain.invoke({\"history\": history, \"input\": input})\n",
    "history.extend([HumanMessage(content=input), ai_msg])\n",
    "print(ai_msg.content)\n",
    "print(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c79d07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = \"please, always tell my name in the beginning of the response... ok?\"\n",
    "ai_msg = chain.invoke({\"history\": history, \"input\": input})\n",
    "history.extend([HumanMessage(content=input), ai_msg])\n",
    "print(ai_msg.content)\n",
    "print(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1eba13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = \"now divide that last result by 3\"\n",
    "ai_msg = chain.invoke({\"history\": history, \"input\": input})\n",
    "history.extend([HumanMessage(content=input), ai_msg])\n",
    "print(ai_msg.content)\n",
    "print(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "46744b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "history1 = []\n",
    "\n",
    "def invoke1(input):\n",
    "    ai_msg = chain.invoke({\"history\": history1, \"input\": input})\n",
    "    history1.extend([HumanMessage(content=input), ai_msg])\n",
    "    return ai_msg.content\n",
    "    \n",
    "history2 = []\n",
    "\n",
    "def invoke2(input):\n",
    "    ai_msg = chain.invoke({\"history\": history2, \"input\": input})\n",
    "    history2.extend([(\"human\", input), (\"ai\", ai_msg.content)])\n",
    "    return ai_msg.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e505f9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(invoke1(\"what's 5 + 2\"))\n",
    "print(invoke1(\"now multiply that by 4\"))\n",
    "print(invoke1(\"by the way, my name is Hammer ... what's your name?\"))\n",
    "print(invoke1(\"what was the last math operation we did?\"))\n",
    "print(invoke1(\"please, always tell my name in the beginning of the response... ok?\"))\n",
    "\n",
    "from langchain.globals import set_debug\n",
    "set_debug(True)\n",
    "print(invoke1( \"now divide that last result by 3\"))\n",
    "set_debug(False)\n",
    "\n",
    "print(history1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c185be",
   "metadata": {},
   "outputs": [],
   "source": [
    "for msg in history1:\n",
    "    if isinstance(msg, HumanMessage):\n",
    "        print(\"human: \", msg.content)\n",
    "    else:\n",
    "        print(\"ai...: \", msg.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029fa3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(invoke2(\"what's 5 + 2\"))\n",
    "print(invoke2(\"now multiply that by 4\"))\n",
    "print(invoke2(\"by the way, my name is Hammer ... what's your name?\"))\n",
    "print(invoke2(\"what was the last math operation we did?\"))\n",
    "print(invoke2(\"please, always tell my name in the beginning of the response... ok?\"))\n",
    "\n",
    "from langchain.globals import set_debug\n",
    "set_debug(True)\n",
    "print(invoke2( \"now divide that last result by 3\"))\n",
    "set_debug(False)\n",
    "\n",
    "print(history2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e082f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "for msg in history2:\n",
    "    print(msg[0], \": \", msg[1])   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8aa2c3",
   "metadata": {},
   "source": [
    "### Let's use RunnableWithMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67670a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create the InMemoryHistory from:\n",
    "# https://python.langchain.com/v0.2/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html\n",
    "\n",
    "from typing import List\n",
    "\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.messages import BaseMessage\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class InMemoryHistory(BaseChatMessageHistory, BaseModel):\n",
    "    \"\"\"In memory implementation of chat message history.\"\"\"\n",
    "\n",
    "    messages: List[BaseMessage] = Field(default_factory=list)\n",
    "\n",
    "    def add_messages(self, messages: List[BaseMessage]) -> None:\n",
    "        \"\"\"Add a list of messages to the store\"\"\"\n",
    "        self.messages.extend(messages)\n",
    "\n",
    "    def clear(self) -> None:\n",
    "        self.messages = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08186d7b",
   "metadata": {},
   "source": [
    "### session by session_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "53c1300d",
   "metadata": {},
   "outputs": [],
   "source": [
    "store = {}\n",
    "\n",
    "def get_by_session_id(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryHistory()\n",
    "    return store[session_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "939825db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f8b52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = get_by_session_id(\"1\")\n",
    "history.add_message(AIMessage(content=\"hello\"))\n",
    "print(store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d41e0b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a valuable assistant.\"),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{input}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | ChatOpenAI() | StrOutputParser()\n",
    "\n",
    "chain_with_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_by_session_id,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"history\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93a2d510",
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke(input):\n",
    "    ai_msg = chain_with_history.invoke(\n",
    "        {\"input\": input}, \n",
    "        config={\"configurable\": {\"session_id\": \"foo\"}})\n",
    "    return ai_msg\n",
    "\n",
    "def show(history):\n",
    "    for msg in history.messages:\n",
    "        if isinstance(msg, HumanMessage):\n",
    "            print(\"human: \", msg.content)\n",
    "        else:\n",
    "            print(\"ai...: \", msg.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8ebb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(invoke(\"what's 5 + 2\"))\n",
    "print(invoke(\"now multiply that by 4\"))\n",
    "print(invoke(\"by the way, my name is Hammer ... what's your name?\"))\n",
    "print(invoke(\"what was the last math operation we did?\"))\n",
    "print(invoke(\"please, always tell my name in the beginning of the response... ok?\"))\n",
    "\n",
    "from langchain.globals import set_debug\n",
    "set_debug(True)\n",
    "print(invoke( \"now divide that last result by 3\"))\n",
    "set_debug(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304efdfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "show(get_by_session_id(\"foo\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b0ed1a",
   "metadata": {},
   "source": [
    "### Session by user_id + conversation_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e1de865",
   "metadata": {},
   "outputs": [],
   "source": [
    "store = {}\n",
    "\n",
    "def get_session_history(user_id: str, conversation_id: str) -> BaseChatMessageHistory:\n",
    "    if (user_id, conversation_id) not in store:\n",
    "        store[(user_id, conversation_id)] = InMemoryHistory()\n",
    "    return store[(user_id, conversation_id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db24677d",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [],
   "source": [
    "history = get_session_history(\"1\", \"foo\")\n",
    "history.add_message(AIMessage(content=\"hello\"))\n",
    "print(store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc3ef937",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_core.runnables import ConfigurableFieldSpec\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a valuable assistant.\"),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{input}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | ChatOpenAI() | StrOutputParser()\n",
    "\n",
    "chain_with_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history=get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"history\",    \n",
    "    history_factory_config=[\n",
    "        ConfigurableFieldSpec(            \n",
    "            id=\"user_id\",\n",
    "            annotation=str,\n",
    "            name=\"User ID\",\n",
    "            description=\"Unique identifier for the user.\",\n",
    "            default=\"\",\n",
    "            is_shared=True,\n",
    "        ),\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"conversation_id\",\n",
    "            annotation=str,\n",
    "            name=\"Conversation ID\",\n",
    "            description=\"Unique identifier for the conversation.\",\n",
    "            default=\"\",\n",
    "            is_shared=True,\n",
    "        ),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "acf3339a",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def invoke(input):\n",
    "    ai_msg = chain_with_history.invoke(\n",
    "        {\"input\": input}, \n",
    "        config={\"configurable\": {\"user_id\": \"123\", \"conversation_id\": \"1\"}})\n",
    "    return ai_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2529400d",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(invoke(\"what's 5 + 2\"))\n",
    "print(invoke(\"now multiply that by 4\"))\n",
    "print(invoke(\"by the way, my name is Hammer ... what's your name?\"))\n",
    "print(invoke(\"what was the last math operation we did?\"))\n",
    "print(invoke(\"please, always tell my name in the beginning of the response... ok?\"))\n",
    "\n",
    "from langchain.globals import set_debug\n",
    "set_debug(True)\n",
    "print(invoke( \"now divide that last result by 3\"))\n",
    "set_debug(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7307b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5018cb0a",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [],
   "source": [
    "show(get_session_history(\"123\", \"1\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba827aa",
   "metadata": {
    "height": 30
   },
   "source": [
    "### Let's use langgraph..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2c07922b",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install --upgrade --quiet langchain-openai langchain langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52696c8c",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "_ = load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b88fe29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "from langgraph.checkpoint.memory import MemorySaver\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "48690d13",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "# Define a new graph\n",
    "workflow = StateGraph(state_schema=MessagesState)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "85bba1f8",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "# Define a chat model\n",
    "model = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "96a62e89",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "# Define the function that calls the model\n",
    "def call_model(state: MessagesState):\n",
    "    response = model.invoke(state[\"messages\"])\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return {\"messages\": response}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f953ad8d",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "# Define the two nodes we will cycle between\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b9bb2617",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "# Adding memory is straight forward in langgraph!\n",
    "memory = MemorySaver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "be1989bf",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "app = workflow.compile(\n",
    "    checkpointer=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b1f8dca6",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "# The thread id is a unique key that identifies this particular conversation.\n",
    "# We'll just generate a random uuid here.\n",
    "# This enables a single application to manage conversations among multiple users.\n",
    "thread_id = uuid.uuid4()\n",
    "config = {\"configurable\": {\"thread_id\": thread_id}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9120949",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "input_message = HumanMessage(content=\"hi! I'm bob\")\n",
    "for event in app.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55849fb5",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "# Here, let's confirm that the AI remembers our name!\n",
    "input_message = HumanMessage(content=\"what was my name?\")\n",
    "for event in app.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3dc109",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.get_tuple(config)\n",
    "\n",
    "# CheckpointTuple(\n",
    "#     config={\n",
    "#         'configurable': {\n",
    "#             'thread_id': UUID('4a6c9b5a-25e2-46ee-9581-752dff4652da'), \n",
    "#             'checkpoint_ns': '', \n",
    "#             'checkpoint_id': '1ef90824-f9da-6de8-8004-09e3403e6e44'\n",
    "#         }\n",
    "#     }, \n",
    "#     checkpoint={\n",
    "#         'v': 1, \n",
    "#         'ts': '2024-10-22T14:31:22.765628+00:00', \n",
    "#         'id': '1ef90824-f9da-6de8-8004-09e3403e6e44', \n",
    "#         'channel_values': {\n",
    "#             'messages': [\n",
    "#                 HumanMessage(\n",
    "#                     content=\"hi! I'm bob\", \n",
    "#                     additional_kwargs={}, \n",
    "#                     response_metadata={}, \n",
    "#                     id='12cbf079-d77d-4645-9239-26aabd9efbe6'\n",
    "#                 ), \n",
    "#                 AIMessage(\n",
    "#                     content='Hello Bob! How can I assist you today?', \n",
    "#                     additional_kwargs={'refusal': None}, \n",
    "#                     response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 12, 'total_tokens': 22, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, \n",
    "#                     id='run-1c31d2a6-b874-4323-bd2d-d7e82066648c-0', \n",
    "#                     usage_metadata={'input_tokens': 12, 'output_tokens': 10, 'total_tokens': 22, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}}\n",
    "#                 ), \n",
    "#                 HumanMessage(\n",
    "#                     content='what was my name?', \n",
    "#                     additional_kwargs={}, \n",
    "#                     response_metadata={}, \n",
    "#                     id='e7315d54-bb65-42a6-830d-3c25c8728dc5'\n",
    "#                 ), \n",
    "#                 AIMessage(\n",
    "#                     content='Your name is Bob. How can I help you today, Bob?', \n",
    "#                     additional_kwargs={'refusal': None}, \n",
    "#                     response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 35, 'total_tokens': 49, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, \n",
    "#                     id='run-ccf50fac-66b8-4a63-811f-4d387104f88a-0', \n",
    "#                     usage_metadata={'input_tokens': 35, 'output_tokens': 14, 'total_tokens': 49, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}}\n",
    "#                 )\n",
    "#             ], \n",
    "#             'model': 'model'\n",
    "#         }, \n",
    "#         'channel_versions': {'__start__': '00000000000000000000000000000005.0.7036559427277947', 'messages': '00000000000000000000000000000006.0.406454342101203', 'start:model': '00000000000000000000000000000006.0.34499960472913005', 'model': '00000000000000000000000000000006.0.8466820891767509'}, \n",
    "#         'versions_seen': {'__input__': {}, '__start__': {'__start__': '00000000000000000000000000000004.0.6525279133846656'}, 'model': {'start:model': '00000000000000000000000000000005.0.22346375696346754'}}, \n",
    "#         'pending_sends': []\n",
    "#     }, \n",
    "#     metadata={\n",
    "#         'source': 'loop', \n",
    "#         'writes': {\n",
    "#             'model': {\n",
    "#                 'messages': AIMessage(\n",
    "#                     content='Your name is Bob. How can I help you today, Bob?', \n",
    "#                     additional_kwargs={'refusal': None}, \n",
    "#                     response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 35, 'total_tokens': 49, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, \n",
    "#                     id='run-ccf50fac-66b8-4a63-811f-4d387104f88a-0', \n",
    "#                     usage_metadata={'input_tokens': 35, 'output_tokens': 14, 'total_tokens': 49, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}}\n",
    "#                 )\n",
    "#             }\n",
    "#         }, \n",
    "#         'step': 4, \n",
    "#         'parents': {}\n",
    "#     }, \n",
    "#     parent_config={\n",
    "#         'configurable': {\n",
    "#             'thread_id': UUID('4a6c9b5a-25e2-46ee-9581-752dff4652da'), \n",
    "#             'checkpoint_ns': '', \n",
    "#             'checkpoint_id': '1ef90824-f25b-6342-8003-87612fbb3965'\n",
    "#         }\n",
    "#     }, \n",
    "#     pending_writes=[]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "94ae491b",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "def invoke(input):\n",
    "    input_message = HumanMessage(content=input)\n",
    "    for event in app.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n",
    "        event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c3aec3",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "invoke(\"what's 5 + 2\")\n",
    "invoke(\"now multiply that by 4\")\n",
    "invoke(\"by the way, my name is Hammer ... what's your name?\")\n",
    "invoke(\"what was the last math operation we did?\")\n",
    "invoke(\"please, always tell my name in the beginning of the response... ok?\")\n",
    "\n",
    "from langchain.globals import set_debug\n",
    "set_debug(True)\n",
    "invoke( \"now divide that last result by 3\")\n",
    "set_debug(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae51a3f",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "input_message = HumanMessage(content=\"thanks\")\n",
    "rsp = app.invoke({\"messages\": [input_message]}, config);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06894001",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "for msg in rsp['messages']:\n",
    "    msg.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f925c91",
   "metadata": {},
   "source": [
    "### LangGraph and Agents?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "76df6084",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.tools import tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.prebuilt import create_react_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2352bec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def get_user_age(name: str) -> str:\n",
    "    \"\"\"Use this tool to find the user's age.\"\"\"\n",
    "    # This is a placeholder for the actual implementation\n",
    "    if \"bob\" in name.lower():\n",
    "        return \"42 years old\"\n",
    "    return \"41 years old\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bf310e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = MemorySaver()\n",
    "model = ChatOpenAI()\n",
    "app = create_react_agent(\n",
    "    model,\n",
    "    tools=[get_user_age],\n",
    "    checkpointer=memory,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9184f2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The thread id is a unique key that identifies\n",
    "# this particular conversation.\n",
    "# We'll just generate a random uuid here.\n",
    "# This enables a single application to manage conversations among multiple users.\n",
    "thread_id = uuid.uuid4()\n",
    "config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "\n",
    "# Tell the AI that our name is Bob, and ask it to use a tool to confirm\n",
    "# that it's capable of working like an agent.\n",
    "input_message = HumanMessage(content=\"hi! I'm bob. What is my age?\")\n",
    "\n",
    "for event in app.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n",
    "    event[\"messages\"][-1].pretty_print()\n",
    "\n",
    "# Confirm that the chat bot has access to previous conversation\n",
    "# and can respond to the user saying that the user's name is Bob.\n",
    "input_message = HumanMessage(content=\"do you remember my name?\")\n",
    "\n",
    "for event in app.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2100d80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tuple = memory.get_tuple(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e327952",
   "metadata": {},
   "outputs": [],
   "source": [
    "for msg in tuple.checkpoint['channel_values']['messages']:\n",
    "    msg.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec16a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "thread_id = uuid.uuid4()\n",
    "config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "\n",
    "\n",
    "from langchain.globals import set_debug\n",
    "set_debug(True)\n",
    "\n",
    "input_message = HumanMessage(content=\"hi! I'm bob. What is my age?\")\n",
    "for event in app.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n",
    "    event[\"messages\"][-1].pretty_print()\n",
    "\n",
    "input_message = HumanMessage(content=\"do you remember my name?\")\n",
    "for event in app.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n",
    "    event[\"messages\"][-1].pretty_print()\n",
    "\n",
    "set_debug(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d877d88e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
